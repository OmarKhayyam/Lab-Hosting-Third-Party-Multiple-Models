{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5aad044",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import boto3\n",
    "import tarfile\n",
    "import os\n",
    "import json\n",
    "import sagemaker\n",
    "from botocore.client import ClientError\n",
    "from sagemaker import image_uris, get_execution_role\n",
    "from time import gmtime, strftime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2e7ac0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-11-04 15:28:38--  https://download.pytorch.org/models/alexnet-owt-7be5be79.pth\n",
      "Resolving download.pytorch.org (download.pytorch.org)... 18.160.37.73, 18.160.37.47, 18.160.37.94, ...\n",
      "Connecting to download.pytorch.org (download.pytorch.org)|18.160.37.73|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 244408911 (233M) [application/x-www-form-urlencoded]\n",
      "Saving to: ‘alexnet-owt-7be5be79.pth’\n",
      "\n",
      "100%[======================================>] 244,408,911  320MB/s   in 0.7s   \n",
      "\n",
      "2022-11-04 15:28:38 (320 MB/s) - ‘alexnet-owt-7be5be79.pth’ saved [244408911/244408911]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://download.pytorch.org/models/alexnet-owt-7be5be79.pth\n",
    "!mkdir -p data/alexnet 2> /dev/null\n",
    "!mv alexnet-owt-7be5be79.pth data/alexnet/\n",
    "## !mv data/alexnet/alexnet-owt-7be5be79.pth data/alexnet/alexnet-owt-7be5be79.pt\n",
    "!mkdir docker 2> /dev/null\n",
    "!cp imagenet_classes.txt docker/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33276397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting docker/entrypoint.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile docker/entrypoint.py\n",
    "import os\n",
    "import shlex\n",
    "import subprocess\n",
    "import sys\n",
    "from subprocess import CalledProcessError\n",
    "\n",
    "from retrying import retry\n",
    "from sagemaker_inference import model_server\n",
    "\n",
    "\n",
    "def _retry_if_error(exception):\n",
    "    return isinstance(exception, CalledProcessError or OSError)\n",
    "\n",
    "\n",
    "@retry(stop_max_delay=1000 * 50, retry_on_exception=_retry_if_error)\n",
    "def _start_mms():\n",
    "    # by default the number of workers per model is 1, but we can configure it through the\n",
    "    # environment variable below if desired.\n",
    "    # os.environ['SAGEMAKER_MODEL_SERVER_WORKERS'] = '2'\n",
    "    model_server.start_model_server(handler_service=\"/home/model-server/model_handler.py:handle\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    if sys.argv[1] == \"serve\":\n",
    "        _start_mms()\n",
    "    else:\n",
    "        subprocess.check_call(shlex.split(\" \".join(sys.argv[1:])))\n",
    "\n",
    "    # prevent docker exit\n",
    "    subprocess.call([\"tail\", \"-f\", \"/dev/null\"])\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "666fe486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting docker/model_handler.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile docker/model_handler.py\n",
    "\"\"\"\n",
    "ModelHandler defines an example model handler for load and inference requests for MXNet CPU models\n",
    "\"\"\"\n",
    "from sagemaker_inference import content_types, decoder, default_inference_handler, encoder, errors\n",
    "from PIL import Image\n",
    "from torchvision import transforms, models\n",
    "import os\n",
    "import io\n",
    "import logging\n",
    "import json\n",
    "import torch\n",
    "\n",
    "\n",
    "class ModelHandler(object):\n",
    "    \"\"\"\n",
    "    A sample Model handler implementation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.initialized = False\n",
    "        self.py_model = None\n",
    "        self.transform = None\n",
    "\n",
    "    def initialize(self, context):\n",
    "        \"\"\"\n",
    "        Initialize model. This will be called during model loading time\n",
    "        :param context: Initial context contains model server system properties.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        logging.info('initialize')\n",
    "        self.initialized = True\n",
    "        properties = context.system_properties\n",
    "        # Contains the url parameter passed to the load request\n",
    "        model_dir = properties.get(\"model_dir\")\n",
    "\n",
    "        # Load PyTorch model\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.py_model = models.alexnet()\n",
    "        state_dict = torch.load(os.path.join(model_dir, 'alexnet-owt-7be5be79.pth'))\n",
    "        self.py_model.load_state_dict(state_dict)\n",
    "        self.py_model.eval()\n",
    "        \n",
    "        # Setup transformations\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225]\n",
    "            )])\n",
    "\n",
    "    def preprocess(self, request):\n",
    "        \"\"\"\n",
    "        Transform raw input into model input data.\n",
    "        :param request: list of raw requests\n",
    "        :return: list of preprocessed model input data\n",
    "        \"\"\"\n",
    "        # Take the input data and pre-process it make it inference ready\n",
    "        # Read the bytearray of the image from the input\n",
    "        logging.info('preprocess')\n",
    "        #logging.info(request)\n",
    "        img = Image.open(io.BytesIO(request[0]['body'])) ## We always assume that this is one image\n",
    "        img_t = self.transform(img)\n",
    "        batch_t = torch.unsqueeze(img_t, 0)\n",
    "        return [batch_t]\n",
    "\n",
    "    def inference(self, model_input):\n",
    "        \"\"\"\n",
    "        Internal inference methods\n",
    "        :param model_input: transformed model input data list\n",
    "        :return: list of inference output in NDArray\n",
    "        \"\"\"\n",
    "        logging.info('preprocess')\n",
    "        return [self.py_model(model_input[0])]\n",
    "\n",
    "    def postprocess(self, inference_output):\n",
    "        \"\"\"\n",
    "        Return predict result in as list.\n",
    "        :param inference_output: list of inference output\n",
    "        :return: list of predict results\n",
    "        \"\"\"\n",
    "        logging.info('postprocess')\n",
    "        with open('/imagenet_classes.txt') as f:\n",
    "            classes = [line.strip() for line in f.readlines()]\n",
    "        _, index = torch.max(inference_output[0], 1)\n",
    "        #percentage = torch.nn.functional.softmax(inference_output[0], dim=1)[0] * 100\n",
    "        ## Get the most likely class\n",
    "        mostlikely = [classes[index[0]]]\n",
    "        logging.info(mostlikely)\n",
    "        return [mostlikely]\n",
    "\n",
    "    def handle(self, data, context):\n",
    "        \"\"\"\n",
    "        Call preprocess, inference and post-process functions\n",
    "        :param data: input data\n",
    "        :param context: mms context\n",
    "        \"\"\"\n",
    "        logging.info('handle')\n",
    "        model_input = self.preprocess(data)\n",
    "        model_out = self.inference(model_input)\n",
    "        return self.postprocess(model_out)\n",
    "\n",
    "\n",
    "_service = ModelHandler()\n",
    "\n",
    "\n",
    "def handle(data, context):\n",
    "    logging.info('handle')\n",
    "    if not _service.initialized:\n",
    "        _service.initialize(context)\n",
    "\n",
    "    if data is None:\n",
    "        return None\n",
    "\n",
    "    return _service.handle(data, context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24573bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting docker/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile docker/Dockerfile\n",
    "FROM ubuntu:18.04\n",
    "\n",
    "# Set a docker label to advertise multi-model support on the container\n",
    "LABEL com.amazonaws.sagemaker.capabilities.multi-models=true\n",
    "# Set a docker label to enable container to use SAGEMAKER_BIND_TO_PORT environment variable if present\n",
    "LABEL com.amazonaws.sagemaker.capabilities.accept-bind-to-port=true\n",
    "\n",
    "\n",
    "# Upgrade installed packages\n",
    "RUN apt-get update && apt-get upgrade -y && apt-get clean\n",
    "\n",
    "# Python package management and basic dependencies\n",
    "RUN apt-get install -y curl python3.7 python3.7-dev python3.7-distutils\n",
    "\n",
    "# Register the version in alternatives\n",
    "RUN update-alternatives --install /usr/bin/python python /usr/bin/python3.7 1\n",
    "\n",
    "# Set python 3 as the default python\n",
    "RUN update-alternatives --set python /usr/bin/python3.7\n",
    "\n",
    "# Install necessary dependencies for MMS and SageMaker Inference Toolkit\n",
    "RUN apt-get -y install --no-install-recommends \\\n",
    "    build-essential \\\n",
    "    ca-certificates \\\n",
    "    openjdk-8-jdk-headless \\\n",
    "    curl \\\n",
    "    vim \\\n",
    "    && rm -rf /var/lib/apt/lists/* \\\n",
    "    && python --version \\\n",
    "    && curl -O https://bootstrap.pypa.io/get-pip.py \\\n",
    "    && python get-pip.py\n",
    "\n",
    "RUN update-alternatives --install /usr/bin/python python /usr/bin/python3 1\n",
    "RUN update-alternatives --install /usr/local/bin/pip pip /usr/local/bin/pip3 1\n",
    "\n",
    "# Install MMS, and SageMaker Inference Toolkit to set up MMS\n",
    "RUN pip3 --no-cache-dir install multi-model-server \\\n",
    "                                sagemaker-inference \\\n",
    "                                retrying\n",
    "\n",
    "RUN pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cpu\n",
    "\n",
    "# Copy entrypoint script to the image\n",
    "COPY entrypoint.py /usr/local/bin/entrypoint.py\n",
    "RUN chmod +x /usr/local/bin/entrypoint.py\n",
    "\n",
    "RUN mkdir -p /home/model-server/\n",
    "\n",
    "# Copy the default custom service file to handle incoming data and inference requests\n",
    "COPY model_handler.py /home/model-server/model_handler.py\n",
    "\n",
    "# Copy the imagenet classes, we will need these for postprocessing\n",
    "COPY imagenet_classes.txt /imagenet_classes.txt\n",
    "\n",
    "# Define an entrypoint script for the docker image\n",
    "ENTRYPOINT [\"python\", \"/usr/local/bin/entrypoint.py\"]\n",
    "\n",
    "# Define command to be passed to the entrypoint\n",
    "CMD [\"serve\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef9bce1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting docker/build_and_push.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile docker/build_and_push.sh\n",
    "# The name of our algorithm\n",
    "algorithm_name=sagemaker-workshop-inf\n",
    "\n",
    "cd docker\n",
    "\n",
    "account=$(aws sts get-caller-identity --query Account --output text)\n",
    "\n",
    "# Get the region defined in the current configuration (default to us-west-2 if none defined)\n",
    "region=$(aws configure get region)\n",
    "region=${region:-us-west-2}\n",
    "\n",
    "fullname=\"${account}.dkr.ecr.${region}.amazonaws.com/${algorithm_name}:latest\"\n",
    "\n",
    "# If the repository doesn't exist in ECR, create it.\n",
    "aws ecr describe-repositories --repository-names \"${algorithm_name}\" > /dev/null 2>&1\n",
    "\n",
    "if [ $? -ne 0 ]\n",
    "then\n",
    "    aws ecr create-repository --repository-name \"${algorithm_name}\" > /dev/null\n",
    "fi\n",
    "\n",
    "# Get the login command from ECR and execute it directly\n",
    "# aws ecr get-login-password --region ${region}|docker login --username AWS --password-stdin ${fullname}\n",
    "$(aws ecr get-login --region ${region} --no-include-email)\n",
    "\n",
    "# Get the login command from ECR in order to pull down the SageMaker PyTorch image\n",
    "#$(aws ecr get-login --registry-ids 763104351884 --region ${region} --no-include-email)\n",
    "\n",
    "# Build the docker image locally with the image name and then push it to ECR\n",
    "# with the full name.\n",
    "\n",
    "docker build -t ${algorithm_name} . --build-arg REGION=${region}\n",
    "docker tag ${algorithm_name} ${fullname}\n",
    "\n",
    "docker push ${fullname}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc1fd163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n",
      "Sending build context to Docker daemon   25.6kB\n",
      "Step 1/19 : FROM ubuntu:18.04\n",
      " ---> 71eaf13299f4\n",
      "Step 2/19 : LABEL com.amazonaws.sagemaker.capabilities.multi-models=true\n",
      " ---> Using cache\n",
      " ---> e2588020736f\n",
      "Step 3/19 : LABEL com.amazonaws.sagemaker.capabilities.accept-bind-to-port=true\n",
      " ---> Using cache\n",
      " ---> d3c71f575f5a\n",
      "Step 4/19 : RUN apt-get update && apt-get upgrade -y && apt-get clean\n",
      " ---> Using cache\n",
      " ---> 71de34bdbb79\n",
      "Step 5/19 : RUN apt-get install -y curl python3.7 python3.7-dev python3.7-distutils\n",
      " ---> Using cache\n",
      " ---> 1ca54a57b0cc\n",
      "Step 6/19 : RUN update-alternatives --install /usr/bin/python python /usr/bin/python3.7 1\n",
      " ---> Using cache\n",
      " ---> 83c7496f57fb\n",
      "Step 7/19 : RUN update-alternatives --set python /usr/bin/python3.7\n",
      " ---> Using cache\n",
      " ---> a41806cc2d47\n",
      "Step 8/19 : RUN apt-get -y install --no-install-recommends     build-essential     ca-certificates     openjdk-8-jdk-headless     curl     vim     && rm -rf /var/lib/apt/lists/*     && python --version     && curl -O https://bootstrap.pypa.io/get-pip.py     && python get-pip.py\n",
      " ---> Using cache\n",
      " ---> b13f7e2f73f6\n",
      "Step 9/19 : RUN update-alternatives --install /usr/bin/python python /usr/bin/python3 1\n",
      " ---> Using cache\n",
      " ---> c85d1c142b0f\n",
      "Step 10/19 : RUN update-alternatives --install /usr/local/bin/pip pip /usr/local/bin/pip3 1\n",
      " ---> Using cache\n",
      " ---> d50575fba165\n",
      "Step 11/19 : RUN pip3 --no-cache-dir install multi-model-server                                 sagemaker-inference                                 retrying\n",
      " ---> Using cache\n",
      " ---> 8e4c6c1cb17b\n",
      "Step 12/19 : RUN pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cpu\n",
      " ---> Using cache\n",
      " ---> 460a93f1bde2\n",
      "Step 13/19 : COPY entrypoint.py /usr/local/bin/entrypoint.py\n",
      " ---> Using cache\n",
      " ---> b3a86110cef1\n",
      "Step 14/19 : RUN chmod +x /usr/local/bin/entrypoint.py\n",
      " ---> Using cache\n",
      " ---> af588577b561\n",
      "Step 15/19 : RUN mkdir -p /home/model-server/\n",
      " ---> Using cache\n",
      " ---> a636dfaea820\n",
      "Step 16/19 : COPY model_handler.py /home/model-server/model_handler.py\n",
      " ---> 3fdb435d8376\n",
      "Step 17/19 : COPY imagenet_classes.txt /imagenet_classes.txt\n",
      " ---> 1ab040fdf40c\n",
      "Step 18/19 : ENTRYPOINT [\"python\", \"/usr/local/bin/entrypoint.py\"]\n",
      " ---> Running in eabf7204eadd\n",
      "Removing intermediate container eabf7204eadd\n",
      " ---> 793dcc8daa8c\n",
      "Step 19/19 : CMD [\"serve\"]\n",
      " ---> Running in 114dc41d8016\n",
      "Removing intermediate container 114dc41d8016\n",
      " ---> d635faa4d008\n",
      "[Warning] One or more build-args [REGION] were not consumed\n",
      "Successfully built d635faa4d008\n",
      "Successfully tagged sagemaker-workshop-inf:latest\n",
      "The push refers to repository [684473352813.dkr.ecr.us-east-1.amazonaws.com/sagemaker-workshop-inf]\n",
      "\n",
      "\u001b[1B4a9fcd32: Preparing \n",
      "\u001b[1Be1978a4f: Preparing \n",
      "\u001b[1Be6aa7264: Preparing \n",
      "\u001b[1B17f63fee: Preparing \n",
      "\u001b[1B989f5df3: Preparing \n",
      "\u001b[1Be228d330: Preparing \n",
      "\u001b[1Bb4d30713: Preparing \n",
      "\u001b[1Bcf2aa7d6: Preparing \n",
      "\u001b[1B2dd660ed: Preparing \n",
      "\u001b[1Bdb2fa75c: Preparing \n",
      "\u001b[1Be1f648c9: Preparing \n",
      "\u001b[1B0745004e: Preparing \n",
      "\u001b[1Bd8fe9cd1: Preparing \n",
      "\u001b[1B3fbee395: Preparing \n",
      "\u001b[15Ba9fcd32: Pushed lready exists 8kB\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[4A\u001b[2K\u001b[1A\u001b[2K\u001b[14A\u001b[2K\u001b[15A\u001b[2Klatest: digest: sha256:798653e6c56b8c89fa7b6afd9046e6e6d420faa1cff758623926462a1322fdd7 size: 3459\n"
     ]
    }
   ],
   "source": [
    "!chmod +x docker/build_and_push.sh && docker/build_and_push.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7469d292",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_client = boto3.client(service_name=\"sagemaker\")\n",
    "runtime_sm_client = boto3.client(service_name=\"sagemaker-runtime\")\n",
    "\n",
    "account_id = boto3.client(\"sts\").get_caller_identity()[\"Account\"]\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "bucket = \"sagemaker-workshop-{}-{}\".format(region, account_id)\n",
    "prefix = \"sagemaker-workshop-endpoint\"\n",
    "\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "012f315d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tarfile.open(\"data/alexnet.tar.gz\", \"w:gz\") as tar:\n",
    "    tar.add(\"data/alexnet\", arcname=\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6aeee10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource(\"s3\")\n",
    "try:\n",
    "    s3.meta.client.head_bucket(Bucket=bucket)\n",
    "except ClientError:\n",
    "    s3.create_bucket(Bucket=bucket)\n",
    "\n",
    "models = {\"alexnet.tar.gz\"}\n",
    "\n",
    "for model in models:\n",
    "    key = os.path.join(prefix, model)\n",
    "    with open(\"data/\" + model, \"rb\") as file_obj:\n",
    "        s3.Bucket(bucket).Object(key).upload_fileobj(file_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17e6f6bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model name: sagemaker-workshop2022-11-04-15-29-16\n",
      "Model data Url: https://sagemaker-workshop-us-east-1-684473352813.s3.amazonaws.com/sagemaker-workshop-endpoint/\n",
      "Container image: 684473352813.dkr.ecr.us-east-1.amazonaws.com/sagemaker-workshop-inf:latest\n",
      "Model Arn: arn:aws:sagemaker:us-east-1:684473352813:model/sagemaker-workshop2022-11-04-15-29-16\n"
     ]
    }
   ],
   "source": [
    "model_name = \"sagemaker-workshop\" + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "## You will have to replace this URL with your specific URL, your may look something like shown below\n",
    "# model_url = \"https://sagemaker-workshop-us-east-1-<ACCOUNT_ID>}.s3.amazonaws.com/<YOUR_PREFIX>/\"\n",
    "model_url = \"https://{}.s3.amazonaws.com/{}/\".format(bucket,prefix)\n",
    "container = \"{}.dkr.ecr.{}.amazonaws.com/{}:latest\".format(\n",
    "    account_id, region, \"sagemaker-workshop-inf\"\n",
    ")\n",
    "\n",
    "print(\"Model name: \" + model_name)\n",
    "print(\"Model data Url: \" + model_url)\n",
    "print(\"Container image: \" + container)\n",
    "\n",
    "container = {\"Image\": container, \"ModelDataUrl\": model_url, \"Mode\": \"MultiModel\"}\n",
    "\n",
    "create_model_response = sm_client.create_model(\n",
    "    ModelName=model_name, ExecutionRoleArn=role, Containers=[container]\n",
    ")\n",
    "\n",
    "print(\"Model Arn: \" + create_model_response[\"ModelArn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc7ced57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint config name: sagemaker-WorkshopEndpointConfig-2022-11-04-15-29-19\n",
      "Endpoint config Arn: arn:aws:sagemaker:us-east-1:684473352813:endpoint-config/sagemaker-workshopendpointconfig-2022-11-04-15-29-19\n"
     ]
    }
   ],
   "source": [
    "endpoint_config_name = \"sagemaker-WorkshopEndpointConfig-\" + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "print(\"Endpoint config name: \" + endpoint_config_name)\n",
    "\n",
    "create_endpoint_config_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"InstanceType\": \"ml.m5.xlarge\",\n",
    "#            \"InstanceType\": \"local\",\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"InitialVariantWeight\": 1,\n",
    "            \"ModelName\": model_name,\n",
    "            \"VariantName\": \"AllTraffic-New-3\",\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"Endpoint config Arn: \" + create_endpoint_config_response[\"EndpointConfigArn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "13faaf04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint name: sagemaker-WorkshopEndpoint-2022-11-04-15-29-25\n",
      "Endpoint Arn: arn:aws:sagemaker:us-east-1:684473352813:endpoint/sagemaker-workshopendpoint-2022-11-04-15-29-25\n",
      "Endpoint Status: Creating\n",
      "Waiting for sagemaker-WorkshopEndpoint-2022-11-04-15-29-25 endpoint to be in service...\n"
     ]
    }
   ],
   "source": [
    "endpoint_name = \"sagemaker-WorkshopEndpoint-\" + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "print(\"Endpoint name: \" + endpoint_name)\n",
    "\n",
    "create_endpoint_response = sm_client.create_endpoint(\n",
    "    EndpointName=endpoint_name, EndpointConfigName=endpoint_config_name\n",
    ")\n",
    "print(\"Endpoint Arn: \" + create_endpoint_response[\"EndpointArn\"])\n",
    "\n",
    "resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = resp[\"EndpointStatus\"]\n",
    "print(\"Endpoint Status: \" + status)\n",
    "\n",
    "print(\"Waiting for {} endpoint to be in service...\".format(endpoint_name))\n",
    "waiter = sm_client.get_waiter(\"endpoint_in_service\")\n",
    "waiter.wait(EndpointName=endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1b84dabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"kitten.jpg\",\"rb\") as f:\n",
    "    payload = bytearray(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5e63742b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'[\\n  \"tabby\"\\n]'\n"
     ]
    }
   ],
   "source": [
    "workshop_predictor = sagemaker.predictor.Predictor(endpoint_name,\n",
    "                                                   sagemaker_session=None\n",
    "                                                  )\n",
    "predicted_value = workshop_predictor.predict(payload,target_model=\"alexnet.tar.gz\")\n",
    "print(predicted_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "27adb1d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tabby']\n"
     ]
    }
   ],
   "source": [
    "response = runtime_sm_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType=\"image/jpeg\",\n",
    "    TargetModel=\"alexnet.tar.gz\",  # this is the rest of the S3 path where the model artifacts are located\n",
    "    Body=payload,\n",
    ")\n",
    "\n",
    "print(json.loads(response['Body'].read()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8143cbb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'aec7e3d8-ad8b-4cf2-8361-d7700b3f07aa',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'aec7e3d8-ad8b-4cf2-8361-d7700b3f07aa',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '0',\n",
       "   'date': 'Fri, 04 Nov 2022 15:35:10 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm_client.delete_endpoint(EndpointName=endpoint_name)\n",
    "sm_client.delete_endpoint_config(EndpointConfigName=endpoint_config_name)\n",
    "sm_client.delete_model(ModelName=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dbcfe542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ResponseMetadata': {'RequestId': '7cfcdbc1-fbb5-465d-bde9-155b3104a983', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '7cfcdbc1-fbb5-465d-bde9-155b3104a983', 'x-amzn-invoked-production-variant': 'AllTraffic-New-3', 'date': 'Fri, 04 Nov 2022 15:33:27 GMT', 'content-type': 'application/json', 'content-length': '13'}, 'RetryAttempts': 0}, 'ContentType': 'application/json', 'InvokedProductionVariant': 'AllTraffic-New-3', 'Body': <botocore.response.StreamingBody object at 0x7f02f9d8a970>}\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab3dcfe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p38",
   "language": "python",
   "name": "conda_pytorch_p38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
